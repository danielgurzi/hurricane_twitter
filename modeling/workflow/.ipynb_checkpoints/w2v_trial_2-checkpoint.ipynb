{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd   \n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import re, string \n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_specific = ['hurricaneharvey', 'harvey', 'houston', 'texas','hurricanharvey']\n",
    "# custom_stopwords = custom_words + disaster_specific\n",
    "custom_words = list(set(\n",
    "    list(ENGLISH_STOP_WORDS) + list(stopwords.words('english')) +\n",
    "    ['en', 'amp', 'instagram', 'hurricaneharvey', 'harvey', 'houston', 'texas', 'com', 'county', 'org',\n",
    "     'www', 'https', 'http', 'rt']))\n",
    "def process_data_by_tweet(file):\n",
    "    df = pd.read_file(file)\n",
    "    # convert to lowercase\n",
    "    self._remove_extraneous(df, custom_stopwords)\n",
    "    df['cleaned_tweets'] = self._clean_tweets(df['text'])\n",
    "    df['lemmatized_tweets'] = self._lemmatize_tweets_spacy(df['cleaned_tweets'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in csv and convert each tweet to an item in a list\n",
    "df = pd.read_csv('./data/hurricaneharvey/twitter_retrieval/hurricaneharvey_10000.csv')\n",
    "tweets = [row.split() for row in df['lemmatized_tweets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-57368b36ca74>:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  w2v_model.train(tweets, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)\n"
     ]
    }
   ],
   "source": [
    "# create word2vec model finding vectors for each word in the quorum\n",
    "w2v_model = Word2Vec(sentences=tweets,\n",
    "                     min_count=1,\n",
    "                 window=5,\n",
    "                 size=100,\n",
    "                 workers=4)\n",
    "\n",
    "# w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(tweets, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)\n",
    "w2v_model.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9173\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# calculate the average vector of all words in the tweet and create tweet dictionary as {tweet: vector}\n",
    "vect_score = []\n",
    "for tweet in tweets:\n",
    "    tweet_vect = []\n",
    "    for word in tweet:\n",
    "        # append the word vector from the model into the tweet_vect list. We will Sum these next. \n",
    "        tweet_vect.append(w2v_model.wv[word])\n",
    "    vect_score.append(sum(tweet_vect)/len(tweet_vect))\n",
    "len(vect_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40223265e-01,  2.81652156e-02, -1.09964222e-01, -1.64557770e-01,\n",
       "        1.04905386e-02,  2.73856074e-02,  9.52607840e-02, -1.69634163e-01,\n",
       "        1.28236383e-01, -1.26077458e-01, -3.52558680e-02,  9.53566208e-02,\n",
       "       -1.63610242e-02,  1.46799326e-01, -7.64902607e-02,  4.31606025e-02,\n",
       "        6.00487590e-02,  5.03449142e-02,  5.29862642e-02,  3.94417942e-02,\n",
       "        6.00875989e-02, -6.24612533e-03, -4.95768478e-03,  1.69669002e-01,\n",
       "        2.11169850e-02,  1.13319561e-01, -1.63763031e-01,  5.37509061e-02,\n",
       "       -1.14197629e-02, -2.41173729e-02, -3.25833727e-03,  1.05494156e-03,\n",
       "        2.35415623e-03,  5.86211458e-02, -3.48350815e-02, -1.62090316e-01,\n",
       "       -1.48385838e-01,  1.34264380e-01,  3.92028168e-02, -1.69060137e-02,\n",
       "        1.94213949e-02, -3.07429284e-02,  1.53678328e-01,  8.32565352e-02,\n",
       "        1.07282326e-01,  8.77068564e-02,  1.98589891e-01, -2.76821610e-02,\n",
       "       -9.03183818e-02,  3.98838706e-02, -7.48796165e-02, -1.21097550e-01,\n",
       "        1.37586117e-01, -1.50767565e-01, -1.46577448e-01,  6.56688469e-04,\n",
       "       -1.28578139e-03, -1.87665656e-01,  1.32194832e-01, -6.12415001e-02,\n",
       "        3.08118090e-02, -1.86782330e-01, -9.96319354e-02,  1.40794784e-01,\n",
       "       -8.10269043e-02,  7.91620314e-02,  4.58405316e-02,  3.40973735e-02,\n",
       "       -5.85168265e-02,  5.30912988e-02, -8.89289752e-02,  4.32473943e-02,\n",
       "        1.55576039e-02,  3.66803259e-02, -2.24744007e-01, -1.37374952e-01,\n",
       "       -9.11280513e-02,  8.23229775e-02,  2.55052466e-02,  5.85246943e-02,\n",
       "       -1.55409217e-01,  9.85138193e-02, -1.21187508e-01,  1.97615083e-02,\n",
       "       -3.99480239e-02,  1.16573207e-01,  3.03114355e-02, -6.71647415e-02,\n",
       "        1.17612630e-01, -9.00113955e-02,  1.17602937e-01, -5.12449183e-02,\n",
       "        1.91732213e-01,  1.08656190e-01,  1.05567686e-01,  1.49280650e-05,\n",
       "        4.87570427e-02, -4.44549173e-02,  2.42365047e-01, -1.02509834e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['hurricanharvey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horrible', 'hurricanharvey']\n",
      "['hurricanharvey', 'beaumont']\n",
      "['hurricanharvey', 'hurricaneirma']\n",
      "['hurricanharvey', 'irmarecovery']\n",
      "['hurricaneirma', 'hurricanharvey']\n",
      "['hurrcaneirma', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayerforhurricanharveyvictims']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricaneirma', 'hurricanharvey']\n",
      "['hurricanharvey', 'hurricaneirma']\n",
      "['hurricanharvey', 'portaransas']\n",
      "['awful', 'hurricanharvey']\n",
      "['bioshock', 'hurricanharvey']\n",
      "['hurricanharvey', 'harveyrelief']\n",
      "['migos', 'hurricanharvey']\n",
      "['relief', 'hurricanharvey']\n",
      "['isaiah', 'hurricanharvey']\n",
      "['hurricanharvey', 'lagrangetexas']\n",
      "['prayforhouston', 'hurricanharvey']\n",
      "['check', 'hurricanharvey']\n",
      "['getinvolved', 'hurricanharvey']\n",
      "['joelosteen', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['party', 'home']\n",
      "['hurricanharvey', 'pleasedontcomethisway']\n",
      "['hurricanharvey', 'calebcity']\n",
      "['pray', 'hurricanharvey']\n",
      "['heroes', 'hurricanharvey']\n",
      "['hurricanharvey', 'afterparty']\n",
      "['hurricanharvey', 'animal']\n",
      "['troopsdirect', 'hurricanharvey']\n",
      "['inspire', 'hurricanharvey']\n",
      "['hurricanharvey', 'helpforhouston']\n",
      "['help', 'hurricanharvey']\n",
      "['meme', 'hurricanharvey']\n",
      "['hurricanharvey', 'help']\n",
      "['smile', 'hurricanharvey']\n",
      "['hurricanharvey', 'texasstrong']\n",
      "['hurricaneharveypets', 'hurricanharvey']\n",
      "['unbelievable', 'hurricanharvey']\n",
      "['harveyrescue', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonstrong']\n",
      "['heros', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['deluxecare', 'hurricanharvey']\n",
      "['hurricanharvey', 'wakeupordiesleep']\n",
      "['hurricanharvey', 'fkiron']\n",
      "['hurricanharvey', 'picture']\n",
      "['let', 'hurricanharvey']\n",
      "['share', 'hurricanharvey']\n",
      "['hurricanharvey', 'diabetes']\n",
      "['hurricanharvey', 'rostec']\n",
      "['ditto', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['prayforhouston', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonflood']\n",
      "['hustonstrong', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayersfortexas']\n",
      "['rain', 'hurricanharvey']\n",
      "['dogrescuer', 'hurricanharvey']\n",
      "['problem', 'hurricanharvey']\n",
      "['hurricanharvey', 'today']\n",
      "['hurricaneharvy', 'hurricanharvey']\n",
      "['city', 'hurricanharvey']\n",
      "['savethechildren', 'hurricanharvey']\n",
      "['hero', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonfloods']\n",
      "['hurricanharvey', 'fox']\n",
      "['techmilldenton', 'hurricanharvey']\n",
      "['help', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['commitment', 'hurricanharvey']\n",
      "['hurricanharvey', 'harveyrelief']\n",
      "['think', 'hurricanharvey']\n",
      "['thank', 'hurricanharvey']\n",
      "['hurricanharvey', 'ltc']\n",
      "['hurricanharvey', 'texasflood']\n",
      "['problem', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'islamicreliefusa']\n",
      "['wow', 'hurricanharvey']\n",
      "['hurricanharvey', 'globalgoodemirelief']\n",
      "['donate', 'hurricanharvey']\n",
      "['hurricanharvey', 'repost']\n",
      "['hurricanharvey', 'globalgoodemirelief']\n",
      "['houstonstrong', 'hurricanharvey']\n",
      "['hurricanharvey', 'globalgoodemirelief']\n",
      "['hurricanharvey', 'corpuschristi']\n",
      "['hurricanharvey', 'corpuschristi']\n",
      "['hurricanharvey', 'houstonfloods']\n",
      "['hurricanharvey', 'mexico']\n",
      "['hurricanharvey', 'prayersfortexas']\n",
      "['problem', 'hurricanharvey']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['hurricanharvey', 'help']\n",
      "['hurricanharvey', 'gapinsurance']\n",
      "['hurricanharvey', 'wkyc']\n",
      "['hurricanharvey', 'texasflood']\n",
      "['plz', 'hurricanharvey']\n",
      "['step', 'hurricanharvey']\n",
      "['hurricanharvey', 'muelleriscoming']\n",
      "['help', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['redcross', 'hurricanharvey']\n",
      "['hurricanharvey', 'donaldtrump']\n",
      "['yup', 'hurricanharvey']\n",
      "['help', 'hurricanharvey']\n",
      "['hurricanharvey', 'theresistance']\n",
      "['hero', 'hurricanharvey']\n",
      "['hurricanharvey', 'globe']\n",
      "['hurricanharvey', 'beating']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonfloods']\n",
      "['hurricane', 'hurricanharvey']\n",
      "['hurricanharvey', 'pray']\n",
      "['prayforhouston', 'hurricanharvey']\n",
      "['tobeapartner', 'hurricanharvey']\n",
      "['houstonflooding', 'hurricanharvey']\n",
      "['hurricanharvey', 'abc']\n",
      "['friendswhobecomefamily', 'hurricanharvey']\n",
      "['help', 'hurricanharvey']\n",
      "['loudspike', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['houstonfloods', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['donate', 'hurricanharvey']\n",
      "['hurricanharvey', 'effect']\n",
      "['hurricanharvey', 'helphouston']\n",
      "['texan', 'hurricanharvey']\n",
      "['hurricanharvey', 'affect']\n",
      "['damn', 'hurricanharvey']\n",
      "['bone', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['heartbreaking', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonstrong']\n",
      "['pray', 'hurricanharvey']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'stop']\n",
      "['cattledrive', 'hurricanharvey']\n",
      "['houstonfloods', 'hurricanharvey']\n",
      "['hurricanharvey', 'nurse']\n",
      "['hurricanharvey', 'mexicostepsup']\n",
      "['hurricanharvey', 'houstonflood']\n",
      "['prayer', 'hurricanharvey']\n",
      "['awesome', 'hurricanharvey']\n",
      "['frightening', 'hurricanharvey']\n",
      "['hurricanharvey', 'resist']\n",
      "['texasflood', 'hurricanharvey']\n",
      "['hurricanharvey', 'helpforharvey']\n",
      "['houstonflooding', 'hurricanharvey']\n",
      "['night', 'hurricanharvey']\n",
      "['water', 'hurricanharvey']\n",
      "['texasflood', 'hurricanharvey']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonflood']\n",
      "['sad', 'hurricanharvey']\n",
      "['hope', 'hurricanharvey']\n",
      "['hurricanharvey', 'trumprussiacollusion']\n",
      "['area', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'geoengineering']\n",
      "['l', 'hurricanharvey']\n",
      "['wow', 'hurricanharvey']\n",
      "['incredible', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayforhouston']\n",
      "['hurricanharvey', 'texasflood']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'houstonflood']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['hurricaneharvery', 'hurricanharvey']\n",
      "['thank', 'hurricanharvey']\n",
      "['fema', 'hurricanharvey']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['houstonflooding', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['houstonflooding', 'hurricanharvey']\n",
      "['hurricanharvey', 'call']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['hurricanharvey', 'texasstrong']\n",
      "['stop', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['thread', 'hurricanharvey']\n",
      "['pic', 'hurricanharvey']\n",
      "['houstonfloods', 'hurricanharvey']\n",
      "['hurricanharvey', 'stormharvey']\n",
      "['hurricanharvey', 'help']\n",
      "['impressive', 'hurricanharvey']\n",
      "['edmorrissey', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayer']\n",
      "['hurricanharvey', 'word']\n",
      "['harris', 'hurricanharvey']\n",
      "['hurricanharvey', 'sprinx']\n",
      "['hurricanharvey', 'sigh']\n",
      "['hurricanharvey', 'update']\n",
      "['hurricanharvey', 'houstonflood']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['damn', 'hurricanharvey']\n",
      "['houstonflood', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['staysafe', 'hurricanharvey']\n",
      "['sharvey', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['omg', 'hurricanharvey']\n",
      "['hurricanharvey', 'stayput']\n",
      "['street', 'hurricanharvey']\n",
      "['prayforthegulf', 'hurricanharvey']\n",
      "['hurricanharvey', 'survivortexasstyle']\n",
      "['hurricanharvey', 'foxnews']\n",
      "['hurricanharvey', 'lol']\n",
      "['send', 'hurricanharvey']\n",
      "['hurricanharvey', 'detail']\n",
      "['lightning', 'hurricanharvey']\n",
      "['onlyintx', 'hurricanharvey']\n",
      "['lookie', 'hurricanharvey']\n",
      "['hurricanharvey', 'life']\n",
      "['help', 'hurricanharvey']\n",
      "['realdonaldtrump', 'hurricanharvey']\n",
      "['mcgregormayweather', 'hurricanharvey']\n",
      "['hurricanharvey', 'preppin']\n",
      "['life', 'hurricanharvey']\n",
      "['help', 'hurricanharvey']\n",
      "['hurricanharvey', 'update']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['hurricanharvey', 'pray']\n",
      "['hurricanharvey', 'people']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'scarey']\n",
      "['hurricanharvey', 'safe']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['pretty', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'spell']\n",
      "['pray', 'hurricanharvey']\n",
      "['horrible', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['safe', 'hurricanharvey']\n",
      "['forever', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['news', 'hurricanharvey']\n",
      "['hurricaneharvery', 'hurricanharvey']\n",
      "['insane', 'hurricanharvey']\n",
      "['hurricanharvey', 'joke']\n",
      "['hurricanharvey', 'thamendment']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'foxnews']\n",
      "['lol', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['get', 'hurricanharvey']\n",
      "['prayersfortexas', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'come']\n",
      "['mistake', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['horrible', 'hurricanharvey']\n",
      "['game', 'hurricanharvey']\n",
      "['eye', 'hurricanharvey']\n",
      "['yesterday', 'hurricanharvey']\n",
      "['course', 'hurricanharvey']\n",
      "['wait', 'hurricanharvey']\n",
      "['yup', 'hurricanharvey']\n",
      "['long', 'hurricanharvey']\n",
      "['thought', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['wow', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['payer', 'hurricanharvey']\n",
      "['hurricanharvey', 'mayweathermcgregor']\n",
      "['womensmonth', 'hurricanharvey']\n",
      "['priority', 'hurricanharvey']\n",
      "['go', 'hurricanharvey']\n",
      "['new', 'hurricanharvey']\n",
      "['hurricanharvey', 'ramrahimsingh']\n",
      "['hurricanharvey', 'ramrahimsingh']\n",
      "['hurricanharvey', 'away']\n",
      "['big', 'hurricanharvey']\n",
      "['staysafe', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['resource', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['prayersfortexas', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['ride', 'hurricanharvey']\n",
      "['hurricanharvey', 'alligator']\n",
      "['pray', 'hurricanharvey']\n",
      "['think', 'hurricanharvey']\n",
      "['abovethenoise', 'hurricanharvey']\n",
      "['amen', 'hurricanharvey']\n",
      "['hurricanharvey', 'rockport']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'iraq']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['gosh', 'hurricanharvey']\n",
      "['hurricanharvey', 'hurricanharvery']\n",
      "['prayer', 'hurricanharvey']\n",
      "['okay', 'hurricanharvey']\n",
      "['myradar', 'hurricanharvey']\n",
      "['problem', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['hurricanharvey', 'fuck']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'pl']\n",
      "['hurricanharvey', 'arpaio']\n",
      "['pray', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'unfortunate']\n",
      "['hurricanharvey', 'poser']\n",
      "['watch', 'hurricanharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['joy', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'breadtwitter']\n",
      "['come', 'hurricanharvey']\n",
      "['category', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['late', 'hurricanharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['hurricanharvey', 'spellcheck']\n",
      "['hurricanharvey', 'corpuschristi']\n",
      "['fuck', 'hurricanharvey']\n",
      "['bring', 'hurricanharvey']\n",
      "['currently', 'hurricanharvey']\n",
      "['hurricanharvey', 'changementclimatique']\n",
      "['fuck', 'hurricanharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['careful', 'hurricanharvey']\n",
      "['come', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['itsrealbad', 'hurricanharvey']\n",
      "['prayer', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayingfortexas']\n",
      "['accurate', 'hurricanharvey']\n",
      "['send', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayfortexas']\n",
      "['hurricanharvey', 'category']\n",
      "['hurricanharvey', 'harveystorm']\n",
      "['hurricanharvey', 'prayersfortexas']\n",
      "['problem', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['problem', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayer']\n",
      "['kill', 'hurricanharvey']\n",
      "['hurricanharvey', 'hurricanemattharvey']\n",
      "['hurricanharvey', 'category']\n",
      "['hurricanharvey', 'hurricane']\n",
      "['hurricanharvey', 'harveystorm']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'like']\n",
      "['safe', 'hurricanharvey']\n",
      "['update', 'hurricanharvey']\n",
      "['hurricanharvey', 'thought']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['hurricanharvey', 'makeamericaspellagain']\n",
      "['pray', 'hurricanharvey']\n",
      "['think', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['die', 'hurricanharvey']\n",
      "['hurricanharvey', 'safe']\n",
      "['chris', 'hurricanharvey']\n",
      "['time', 'hurricanharvey']\n",
      "['joke', 'hurricanharvey']\n",
      "['hang', 'hurricanharvey']\n",
      "['smh', 'hurricanharvey']\n",
      "['prayfortexas', 'hurricanharvey']\n",
      "['wait', 'hurricanharvey']\n",
      "['soon', 'hurricanharvey']\n",
      "['hang', 'hurricanharvey']\n",
      "['safe', 'hurricanharvey']\n",
      "['nice', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayersfortexas']\n",
      "['hurricanharvey', 'staywoke']\n",
      "['hurricaineharvey', 'hurricanharvey']\n",
      "['hurricanharvey', 'staysafetexas']\n",
      "['hurricane', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayingfortexas']\n",
      "['wow', 'hurricanharvey']\n",
      "['pray', 'hurricanharvey']\n",
      "['hurricanharvey', 'prayingfortexas']\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    if len(tweet) == 2:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEETS\n",
      "Word Count: 83360\n",
      "Shortest Tweet: 1\n",
      "\n",
      "MODEL\n",
      "Type: <class 'dict'>\n",
      "Length: 14889\n",
      "\n",
      "VECT_SCORE\n",
      "Type: <class 'dict'>\n",
      "Length: 9173\n"
     ]
    }
   ],
   "source": [
    "print('TWEETS')\n",
    "print(f'Word Count: {sum([len(tweet) for tweet in tweets])}')\n",
    "print(f'Shortest Tweet: {min([len(tweet) for tweet in tweets])}')\n",
    "print('\\nMODEL')\n",
    "print(f'Type: {type(w2v_model.wv.vocab)}')\n",
    "print(f'Length: {len(list(w2v_model.wv.vocab))}')\n",
    "print('\\nVECT_SCORE')\n",
    "print(f'Type: {type(vect_score)}')\n",
    "print(f'Length: {len(list(vect_score))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([len(tweet) for tweet in tweets if len(tweet) < 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielgurzi/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3dc9ead11bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mkclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansClusterer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_means\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavoid_empty_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0massigned_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# n_clusters = 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/nltk/cluster/util.py\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, vectors, assign_clusters, trace)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# call abstract method to cluster the vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# assign the vectors to clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36mcluster_vectorspace\u001b[0;34m(self, vectors, trace)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mmeanss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36m_cluster_vectorspace\u001b[0;34m(self, vectors, trace)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_vectorspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/nltk/cluster/kmeans.py\u001b[0m in \u001b[0;36mclassify_vectorspace\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_distance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mbest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/nltk/cluster/util.py\u001b[0m in \u001b[0;36mcosine_distance\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mequal\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Word2Vec into Kmeans from https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/\n",
    "\n",
    "X = w2v_model[w2v_model.wv.vocab]\n",
    "X2 = list(vect_score.values())\n",
    "\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "kclusterer = KMeansClusterer(num_means=20, distance=nltk.cluster.util.cosine_distance, repeats=25,avoid_empty_clusters=True)\n",
    "assigned_clusters = kclusterer.cluster(X2, assign_clusters=True)\n",
    "\n",
    "# n_clusters = 20\n",
    "\n",
    "# kmeans = KMeans(n_clusters=n_clusters).fit(X)\n",
    "\n",
    "# clusters = kmeans.cluster_centers_\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# filename = f\"../data/mendocinocomplex/kmeans/kmeans_{n_clusters}_{current_time}.pkl\"\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(kmeans, file)\n",
    "#     file.close()\n",
    "# df['cluster'] = kmeans.predict(X)\n",
    "# print(df[['cluster', 'id', 'text']])\n",
    "# dataframe.to_csv(f\"../data/mendocinocomplex/kmeans/mendocinocomplex_{n_clusters}_{current_time}.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words from model\n",
    "words = list(w2v_model.wv.vocab)\n",
    "for i, word in enumerate(words):  \n",
    "    print (word + \":\" + str(assigned_clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=20)\n",
    "kmeans.fit(X2)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Create TSNE model and plot it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "   \n",
    "tsne_plot(w2v_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
